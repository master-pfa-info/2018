Introduction to concurrent programming
Master 2

Sebastien Binet
CNRS/IN2P3/LPC-Clermont
sebastien.binet@clermont.in2p3.fr

* Concurrent programming

* Why is concurrency hard?

- race conditions
- atomicity
- memory access synchronization
- deadlocks, livelocks and starvation

* Race conditions

A race condition occurs when 2 or more operations *must* execute in the *correct* order, but that order is not enforced by the program.

.play _code/race-cond.go /START/,/STOP/

* Race conditions

Done: fixed it!

.play _code/race-cond-sleep.go /START/,/STOP/

or did we? well... no.
We've just made it increasingly *unlikely* to have the "bad" behaviour.
We've also made our program less efficient.

Race conditions can be quite painful the bug to track: they may be lurking for years, deep inside a component of a program, and only show up when something else has changed in the environment.

* Race conditions

.image _figs/thread-exec-race.png

For years we could have been in a situation where we'd always get:

  r1 == 1
  r2 == 1

and at some point in time, something changes and we get either _"Execution_ _1"_ or _"Execution_ _2"_...

* Atomicity

An operation is considered *atomic*, or to have the property of *atomicity* when, _within_ _the_ _context_ _that_ _it_ _is_ _operating_, it is *indivisible* or *uninterruptible*.

Depending on the context (a program, the OS, the machine, ...), an operation may or may not retain its property of atomicity.

Consider this instruction:

  i++

It may _look_ atomic.

But it consists in fact in several operations:

- retrieve the value of _i_
- increment the value of _i_ by one
- store the value of _i_.

* Memory access synchronization

Consider the following data race:

.play _code/data-race.go /START/,/STOP/

The output of this program is completely *nondeterministic*.
There are 3 parts in this program that need exclusive access to a shared resource (_data_):

- the goroutine that increments _data_
- the _if_ statement that checks the value of _data_
- the _else_ body that retrieves and prints the value of _data_.

These parts are called *critical* *sections*.

* Memory access synchronization

There are various ways to address this issue and _guard_ a program's critical sections.
One way (a not very Go-ish one) is the following:

.play _code/data-race-wg.go /START/,/STOP/

We protect the access+modification of _data_ with the _mux_ variable.
Access is synchronized, the data race goes away. (but *not* the race condition!)

But if somebody forgets to properly use _Lock()/Unlock()_, all hell breaks loose...

* Deadlock

2 people at a door, trying to enter a room

- Alice waits to let Barbara in
- Barbara waits to let Alice in
- ...

There are a few conditions for deadlocks to arise.
These conditions have been enumerated in 1971 by Edgar Coffman in a [[https://bit.ly/CoffmanDeadlocks][paper]]:

- mutual exclusion
- wait for a condition
- no preemption
- circular wait

* Deadlock

.code _code/deadlock.go /START-VALUE/,/STOP-VALUE/
.play _code/deadlock.go /START-MAIN/,/STOP-MAIN/

* Livelock

2 people in a hallway:

- Alice goes right to let Barbara pass through
- Barbara goes left to let Alice pass through
- Alice scoots left
- Barbara also scoots left
- ...

Incidentally, this simple example demonstrates a very common reason *livelocks* happen: 2 or more concurrent processes attempting to prevent a *deadlock* to happen _without_ _coordination_.

Livelocks are a subset of a larger set of problems called *starvation*.

* Starvation

 Starvation is any situation where a concurrent process cannot get all
 the resources it needs to carry a task.

Livelocks are a special case of starvation where all concurrent processes are starved _equally:_ no work is performed.

Starvation is when one or more concurrent processes are greedy and prevent one or more concurrent processes from acquiring all the resources needed to perform their work as efficiently as possible (or at all.)

_Example:_ people boarding a plane (or a train) trying to greedily store their luggages, preventing other people from getting at their seat.

* Determining concurrency safety

 // CalculatePi calculates digits of Pi between the begin and end place.
 func CalculatePi(begin, end int64, pi *Pi)

- how should I call this concurrently? (should I?)
- am I supposed to concurrently invoke that function?
- am I responsible for synchronizing the access to `*Pi`? (or does this type handle that under the covers?)

* Determining concurrency safety

Documentation does wonders...

 // CalculatePi calculates digits of Pi between the begin and end place.
 //
 // Internally, CalculatePi will create FLOOR((end-begin)/2) concurrent processes
 // which recursively call CalculatePi.
 // Synchronization of writes to pi is handled internally by the Pi struct.
 func CalculatePi(begin, end int64, pi *Pi)

- who is responsible for the concurrency?
- how is the problem space mapped onto concurrency primitives?
- who is responsible for the synchronization?

* Go's concurrency building blocks

`sync` package:

.link https://godoc.org/sync#WaitGroup sync.WaitGroup
.link https://godoc.org/sync#Mutex sync.Mutex
.link https://godoc.org/sync#RWMutex sync.RWMutex
.link https://godoc.org/sync#Cond sync.Cond
.link https://godoc.org/sync#Once sync.Once
.link https://godoc.org/sync#Pool sync.Pool


builtins:

- channels + goroutines
- `select`

* Go concurrency model: fork-join

.play _code/go-fork-join-model.go /START/,/STOP/

.image _figs/fork-join.png _ 400

* Go concurrency model: fork-join

The previous example had one problem:

- there is no guarantee that `sayHello` will manage to execute.

The goroutine may very well be created, scheduled to be executed by the Go runtime, but may never get a chance to actually run before the end of `main`.

To correct this behaviour, we need to create "join" point, so the `main` and `sayHello` goroutines are synchronized.

* Go concurrency model: fork-join

.play _code/go-fork-join-wg.go

Now, the `main` goroutine is *deterministically* blocked, waiting for the `sayHello` goroutine to give the "go-ahead".

It's just one way to achieve synchronization... (we'll see some of those.)

* sync.WaitGroup

.play _code/sync-waitgroup.go /START/,/STOP/

* sync.WaitGroup

.play _code/sync-waitgroup-many.go /START/,/STOP/

* sync.Mutex

.code _code/sync-mutex.go /START-DECL/,/STOP-DECL/

* sync.Mutex

.play _code/sync-mutex.go /START-MAIN/,/STOP-MAIN/

* sync.Mutex

- always make sure each `Mutex.Lock()` has its `Mutex.Unlock()` counterpart
- try to minimize the _"critical_ _section"_ (the code b/w `Lock` and `Unlock`)
- critical sections are bottlenecks in programs

One possible avenue to reduce critical sections is to distinguish b/w exclusive *read* access to a piece of memory and *write* access.

ie: use a [[https://godoc.org/sync#RWMutex][sync.RWMutex]]

## * sync.RWMutex
##
## .code _code/sync-rwmutex.go /START-HEAD/,/STOP-HEAD/
##
## * 
##
## .play _code/sync-rwmutex.go /START-MAIN/,/STOP-MAIN/

* sync.Cond

[[https://godoc.org/sync#Cond][Cond]] is a rendezvous point for goroutines waiting for or announcing the occurrence of an event.
That event can be any arbitrary signal between 2 or more goroutines.
The event itself does not carry any informaton other than "it occurred".

A naive (and CPU hungry) way to do that could be:

  for condition() == false {
      time.Sleep(1*time.Millisecond)
  }

A better way is to use [[https://godoc.org/sync#Cond][sync.Cond]].

* sync.Cond

.code _code/sync-cond.go /START/,/STOP/

This approach is much more efficient:

- _Wait_ not only blocks the goroutine, it _suspends_ it
- this allows other goroutines to run on the OS thread

_NB:_ it looks like we hold the lock the whole time.
In fact, _Wait_ calls _Unlock_ at the start, and calls _Lock_ when it exits.

* sync.Cond

.play _code/sync-cond-queue.go /START/,/STOP/

* sync.Cond

.code _code/sync-cond-button.go /START/,/STOP/

* sync.Cond

.play _code/sync-cond-button.go /START-MAIN/,/STOP-MAIN/

* sync.Once

.play _code/sync-once.go /START/,/STOP/

[[https://godoc.org/sync#Once][sync.Once]] is a type that leverages _sync_ primitives to ensure that only _one_ call to _Do_ ever calls the function passed in, even on different goroutines.

* sync.Once

.play _code/sync-once-gotcha.go /START/,/STOP/

[[https://godoc.org/sync#Once][sync.Once]] only counts the number of times _Do_ is called, *not* how many unique functions are passed to _Do_.

* sync.Pool

[[https://godoc.org/sync#Pool][sync.Pool]] is a concurrent-safe implementation of the object pool pattern.

It provides a way to create and make available a fixed number (a pool) of things for use.
A _sync.Pool_ can be safely used from multiple goroutines.

.play _code/sync-pool.go /START/,/STOP/

Using a pool can reduce the amount of memory to be allocated, and the pressure on the Garbage Collector (and thus improve runtime performances.)

* Concurrency patterns in Go

* Confinement

We have already studied different options for safe operation when working with concurrent code:

- synchronization primitives for sharing memory (_e.g.:_ [[https://godoc.org/sync#Mutex][sync.Mutex]])
- synchronization via communicating (_e.g.:_ channels)

But there are a couple of others:

- immutable data
- data protected by confinement.

*Confinement:* ensuring information is only ever available from *one* concurrent process.
When confinement is realized, a concurrent program is implicitly safe and no synchronization is required.

* Confinement: ad hoc

.play _code/confinement-ad-hoc.go /START/,/STOP/

* Confinement: lexical

.play _code/confinement-lexical.go /START/,/STOP/

* Confinement: lexical - II

.play _code/confinement-lexical-buf.go /START/,/STOP/

Here `process` has only access to a subset of the whole `data` slice.
Each `process` goroutine is given a different subset of the whole `data` slice.
No need for synchronization of memory access or sharing data thru communication.

* The for-select loop

At its core, this idiom is simply:

  for { // either an infinite loop, or range over something
     select {
     // do some work with channels
     }
  }

_e.g.:_

  // sending iteration variables out on a channel
  for _, s := range []string{"a", "b", "c"} {
      select {
      case <-done:
  	    return
      case ch <- s:
      }
  }

* for-select

Looping infinitely, waiting to be stopped:

  for {
      select {
      case <-done:
          return
      default:
      }
  
      // do non-preemptable work
  }

Or:

  for {
      select {
      case <-done:
          return
      default:
          // do non-preemptable work
      }
  }


* Goroutine leaks

Goroutines are _cheap_ and easy to create, but they aren't *free*.
They _do_ _cost_ resources that should be reclaimed at some point during the execution of a program.

Goroutines are not garbage collected by the runtime: we need to make sure they are properly cleaned up.

A Goroutine is terminated:

- when it has completed its work.
- when it cannot continue its work due to to an unrecoverable error.
- when it's told to stop working.

The first 2 cases come _"for_ _free":_ they are just part of the algorithm being implemented.

The last one is about *work* *cancellation*.

* Goroutine leaks

.play _code/goroutine-leak.go /START/,/STOP/

* Goroutine leaks -- fixed

.code _code/goroutine-leak-fixed.go /START-WORK/,/STOP-WORK/

* Goroutine leaks -- fixed

.play _code/goroutine-leak-fixed.go /START-MAIN/,/STOP-MAIN/

* Goroutine leaks

.play _code/goroutine-wleak.go /START-MAIN/,/STOP-MAIN/

Oops, this isn't displayed:

 gen closure exited

The goroutine is leaked. Try to fix it.

## * Solution
## 
## .play _code/goroutine-wleak-fixed.go /START-MAIN/,/STOP-MAIN/
## 

* or-channel

Use case: combine one or more `done` channels into a single `done` channel that closes if any of its component channels close.

* 

.code _code/or-chan.go /^func or/,/^}$/

* 

.play _code/or-chan.go /START/,/STOP/

_Exercize:_ Modify the code to also display the number of goroutines being scheduled.

* Error handling

Error handling, especially in concurrent programs, can be difficult to get right.

The most fundamental question when thinking about error handling is (whether in a concurrent program or not):

 Who should be responsible for handling the error?

One can bubble the error up the call stack, but at some point _somebody_ will have to deal with it.

With concurrent programs, this question becomes a little bit more complex...

* Example

.code _code/err-handling.go /START-WORK/,/STOP-WORK/

* 

.play _code/err-handling.go /START-MAIN/,/STOP-MAIN/

The goroutine displays the errors, hoping _somebody_ looks at the printout (and takes action.)
That is not a robust way to deal with an error.

How would you fix this?

## * Solution
## 
## .code _code/err-handling-fixed.go /START-WORK/,/STOP-WORK/
## 
## * Solution
## 
## .play _code/err-handling-fixed.go /START-MAIN/,/STOP-MAIN/
## 
## Here, we've extracted the error handling from the producer goroutine and relocalized it where it makes more sense.
## It's also where most of the useful context exists (so more intelligent decisions about what to do with the errors can take place.)

* Pipelines

Writing programs is all about chunking work between functions of adequate length:

- we construct abstractions in the form of functions
- build up a basic vocabulary based off functions

This allows to abstract away "uninteresting" details in the grand scheme of the universe.
This allows to work on a restricted area of the whole code, without impacting other areas.

In effect, we build _pipelines_ of functions, to process streams or batches of data.

 +---------+     +---------+     +---------+
 | stage 1 | --> | stage 2 | --> | stage 3 | --> ...
 +---------+     +---------+     +---------+

* Pipelines

.code _code/pipeline-batch.go /START-MUL/,/STOP-MUL/
.code _code/pipeline-batch.go /START-ADD/,/STOP-ADD/

* Pipelines

Let's combine these 2 _add_ and _mul_ functions:

.play _code/pipeline-batch.go /START-MAIN/,/STOP-MAIN/

- each stage (_mul_, _add_) consumes and returns the same type (_ie:_ _[]int_)
- each stage can be passed around (as a value, or as an argument of another stage)

The pipeline above performs a *batch* *processing*: each stage operates on chunks of data all at once, instead of one discrete value at a time.

*stream* *processing* is another type of pipeline where each stage receives or emits one element at a time.

* Pipelines

.play _code/pipeline-stream.go /START-MUL/,/STOP-MAIN/

- each stage is receiving and emitting a discrete value (reduced memory footprint _wrt_ batch processing)
- pipeline is now hard-coded inside the for-loop

This limit the reusability of the pipeline _and_ its ability to scale.

* Concurrent pipelines

Channels are a great support to construct pipelines in Go:

- they can receive and emit values
- they can safely be used concurrently
- they can be _ranged_ over
- they can be passed around (as arguments, values, ...)

Let's try to rewrite our pipeline in terms of channels...

* 

.code _code/pipeline-conc.go /START-GEN/,/STOP-GEN/

* 

.code _code/pipeline-conc.go /START-ADD/,/STOP-ADD/

* 

.code _code/pipeline-conc.go /START-MUL/,/STOP-MUL/

* 

.play _code/pipeline-conc.go /START-MAIN/,/STOP-MAIN/

- we can _range_ to extract the values
- at each stage, we can safely execute concurrently
- each stage only needs to wait for its inputs to be able to send its ouputs
- each stage can thus execute independent of one another, for some slice of time.

* Nifty generators

* Repeat

.code _code/gen-repeat.go /START-REPEAT/,/STOP-REPEAT/

* Take

.code _code/gen-repeat.go /START-REPEAT/,/STOP-REPEAT/

* Example: repeat + take

.play _code/gen-repeat.go /START-MAIN/,/STOP-MAIN/

* Repeat func

.code _code/gen-repeat-func.go /START-REPEAT/,/STOP-REPEAT/
.play _code/gen-repeat-func.go /START-MAIN/,/STOP-MAIN/


* Other patterns

- Fan-in, Fan-out
- or-done-channel
- tee-channel, bridge-channel
- queuing
- ...

For more background:

.link https://blog.golang.org/pipelines
.link https://blog.golang.org/context
.link https://talks.golang.org/2012/waza.slide
.link https://talks.golang.org/2012/splash.slide
.link https://katherine.cox-buday.com/concurrency-in-go/

* Resources

.link https://golang-book.com
.link http://www.inf.unibz.it/dis/teaching/PP/ln/pp01_intro.pdf
.link http://www.inf.unibz.it/dis/teaching/PP/ln/pp02_oo.pdf
.link https://tour.golang.org
.link https://www.golangbootcamp.com/book
.link https://katherine.cox-buday.com/concurrency-in-go/

